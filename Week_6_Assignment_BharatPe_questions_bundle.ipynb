{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asif-Ansari-bbl/AlmaX-Assignment/blob/main/Week_6_Assignment_BharatPe_questions_bundle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Guidelines for assignment:\n",
        "\n",
        "1. Create a copy of this notebook in your drive and write down the answers to each question in your own words. Avoid plagiarism.\n",
        "2. Record a video of yourself answering these questions in a manner, (with your camera on) you would in an interview. The intention is not to read out the answer you have written. Understand the concept and explain it in your words as you would to an interviewer. You can use Zoom or similar applications to record your video.\n",
        "3. Upload the recorded video to your YouTube account as an **unlisted** video and share its link in the notebook along with solutions.\n",
        "You can refer to [this video](https://youtu.be/JOr7JluzEOM) for steps to unlist your video."
      ],
      "metadata": {
        "id": "zRy1zWXy4_VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Case Study Question:**\n",
        "\n",
        "1. Summarise your understanding about the company and its operations. What is the business model here?\n",
        "\n",
        "2. Conduct a SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis of the firm with respect to Fintech  industry.\n",
        "\n",
        "3. What will be your approach to build a fraud detection system to detect fraudulent transactions in BharatPe platform?\n",
        "\n",
        "4. What are the features you would require to design this system?\n",
        "\n",
        "5. How do you measure the performance of the system?"
      ],
      "metadata": {
        "id": "ZaN7y7eFKy0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1 :-Summarise your understanding about the company and its operations. What is the business model here?"
      ],
      "metadata": {
        "id": "wEW_-obKc3Nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BharatPe is India’s fintech company that caters to small merchants and kirana store owners in India . It helps merchants accept payments from customers for free. It also offers fintech products like loans. It is a full-stack service app for merchants across small and medium businesses, offering loans, investments, insurance, and other financial services. BharatPe was co-founded by Ashneer Grover and Shashvat Nakrani in 2018 with the vision to make financial inclusion a reality for Indian merchants. BharatPe gets listed as one of the Top 5 most valued Fintech startups in India.\n",
        "\n",
        "BharatPe operates through a unique business model that combines payments and lending services. The core of their operations revolves around enabling merchants to accept payments from customers using various digital payment methods such as UPI, QR codes, and credit/debit cards. By leveraging technology, BharatPe simplifies the payment process for merchants, enabling them to accept payments seamlessly and manage transactions efficiently.\n",
        "\n",
        "Overall, BharatPe's business model centers around facilitating digital payments and providing convenient lending options to empower SMBs in India. Through their platform, they strive to simplify financial transactions and support the growth of small businesses in the country."
      ],
      "metadata": {
        "id": "BIeXxuFIc9J3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 2:- Conduct a SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis of the firm with respect to Fintech industry."
      ],
      "metadata": {
        "id": "0DtohOEsdeL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A SWOT analysis is a tool use to evaluate a company’s strengths, weaknesses, opportunities, and threats. Here is a brief SWOT analysis of all BharatPe.\n",
        "\n",
        "### **Strengths :-**:\n",
        "\n",
        "* **Seamless Payment Solutions :-** BharatPe offers a user-friendly and seamless QR code-based payment solution that simplifies digital transactions for small merchants. This simplicity and ease of use give the company a competitive advantage.\n",
        "\n",
        "* **Zero Transaction Fees :-** BharatPe’s zero transaction fee model sets it apart from many competitors and attracts small merchants who are looking to minimize costs and maximize profits.\n",
        "\n",
        "*  **Access to Credit :-** BharatPe’s lending services provide small businesses with quick and convenient access to working capital, addressing a significant pain point for merchants in the Fintech industry.\n",
        "\n",
        "* **Strong Market Presence :-** BharatPe has established a strong market presence and brand recognition within a short period. Its growing customer base and partnerships with major financial institutions contribute to its competitive position.\n",
        "\n",
        "### **Weaknesses :-**\n",
        "\n",
        "* **Limited Product Diversification :-** Currently, BharatPe’s primary focus is on QR code-based payments and lending services. The company may face challenges if it doesn’t expand its product offerings to cater to a broader range of merchant needs.\n",
        "\n",
        "* **Reliance on UPI Ecosystem :-** BharatPe’s business model heavily relies on the UPI ecosystem, which can be subject to regulatory changes or disruptions. Overdependence on a single platform could pose risks to its operations.\n",
        "\n",
        "\n",
        "### **Opportunities :-**\n",
        "\n",
        "* **Untapped Market Potential :-** Despite the growth of digital payments in India, there is still a significant opportunity to expand the adoption of digital payment solutions among small merchants. BharatPe can capitalize on this untapped market potential by increasing its market penetration.\n",
        "\n",
        "* **Product and Service Expansion :-** BharatPe can explore opportunities to expand its product and service portfolio to cater to a wider range of merchant needs. This could include introducing additional financial products, integration with third-party services, or exploring new technologies such as blockchain or artificial intelligence.\n",
        "\n",
        "* **Strategic Partnerships :-** Collaborating with banks, financial institutions, and other fintech companies can unlock new opportunities for BharatPe. Partnerships could provide access to new customer segments, technological advancements, and distribution channels.\n",
        "\n",
        "\n",
        "### **Threats :-**\n",
        "\n",
        "* **Intense Competition :-** The fintech industry in India is highly competitive, with numerous players vying for market share. Established players and new entrants pose a threat to BharatPe’s market position and customer acquisition efforts.\n",
        "\n",
        "* **Regulatory Environment :-** The regulatory landscape in the fintech industry is subject to changes and uncertainties. Compliance with evolving regulations and licensing requirements could pose challenges for BharatPe’s operations and growth.\n",
        "* **Technological Advancements :-** Rapid advancements in technology can disrupt the fintech industry. New innovations, payment methods, or platforms may emerge, posing a threat to the existing business models and market position of companies like BharatPe.\n"
      ],
      "metadata": {
        "id": "MoekFaL3djom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3 :- What will be your approach to build a fraud detection system to detect fraudulent transactions in BharatPe platform?"
      ],
      "metadata": {
        "id": "H0xjEWrifC94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Build fraud detection system so that it can detect fraudlent transactions in Baharat pay we have to follow this step  \n",
        "\n",
        "* **Data Collection: -** Gather relevant data from various sources, such as transaction logs, customer profiles, device information, IP addresses, and historical fraud data. This data will serve as the foundation for training and testing the fraud detection models.\n",
        "\n",
        "* **Data Preprocessing: -** Clean and preprocess the collected data to ensure its quality and compatibility for analysis. This step involves tasks such as data validation, handling missing values, data transformation, and feature engineering. Feature engineering may include extracting relevant features like transaction amount, location, time of day, transaction history, and user behavior patterns.\n",
        "\n",
        "* **Feature Selection: -** Identify the most informative and relevant features for fraud detection. Use techniques like correlation analysis, information gain, or feature importance ranking algorithms to select the features that contribute the most to detecting fraudulent transactions.\n",
        "\n",
        "* **Model Selection: -**Choose appropriate machine learning algorithms for building the fraud detection models. Commonly used techniques include supervised learning algorithms such as logistic regression, decision trees, random forests, or gradient boosting. Unsupervised learning techniques like clustering or anomaly detection can also be employed to identify suspicious patterns.\n",
        "\n",
        "* **Model Training: -** Split the preprocessed data into training and validation sets. Use the training set to train the chosen machine learning models. During training, the models learn patterns and features associated with fraudulent transactions. Fine-tune the models using techniques like cross-validation and hyperparameter tuning to optimize their performance.\n",
        "\n",
        "**Model Evaluation: -** Evaluate the trained models using the validation set. Metrics such as precision, recall, F1 score, and accuracy can be used to assess the models' performance. Consider the trade-off between false positives and false negatives based on the system's requirements and the impact of misclassification.\n",
        "\n",
        "**Real-time Monitoring: -** Implement the fraud detection system into the BharatPe platform to continuously monitor transactions in real-time. Develop mechanisms to detect and flag potentially fraudulent transactions based on the trained models and predefined thresholds. Implement alert systems or notification mechanisms to notify relevant personnel about flagged transactions for further investigation.\n",
        "\n",
        "**Ongoing Model Maintenance: -** Regularly update the fraud detection models with new data to adapt to evolving fraud patterns. Monitor the system's performance over time and continuously refine the models to enhance accuracy and reduce false positives/negatives. Stay updated with the latest fraud techniques and industry trends to incorporate new insights into the detection system.\n",
        "\n",
        "**Collaboration and Improvement: -** Foster collaboration between data scientists, fraud analysts, and domain experts to gather insights, exchange knowledge, and improve the fraud detection system iteratively. Regularly review and analyze detected fraud cases to identify new patterns and adjust the system accordingly.\n",
        "\n",
        "One most important thing is building an effective fraud detection system requires continuous learning and improvement, as fraudsters constantly evolve their tactics. Regularly updating and enhancing the system's capabilities will help in staying ahead of new fraud threats and maintaining the security of the BharatPe platform."
      ],
      "metadata": {
        "id": "oXpfk5p_fqX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4:-What are the features you would require to design this system?"
      ],
      "metadata": {
        "id": "yHGWD5Z_hraW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Design a fraud detection system for the BharatPe platform would require a combination of relevant features that can help identify fraudulent transactions like :-\n",
        "\n",
        "* **Transaction Metadata :-** Include transaction-specific information such as transaction amount, timestamp, currency, location, and merchant details. These features provide basic information about each transaction.\n",
        "\n",
        "* **User Profile Information :-** Incorporate user-related features such as user ID, account age, transaction history, average transaction amount, and frequency of transactions. Analyzing user behavior patterns can help identify suspicious activity.\n",
        "\n",
        "* **Device Information :-** Gather data about the device used for the transaction, including device type, operating system, IP address, and geolocation. Sudden changes in device characteristics may indicate fraudulent activity.\n",
        "\n",
        "* **Geolocation Data :-** Capture the location of the transaction and compare it with the user's historical transaction patterns. Unusual or inconsistent location data could be an indicator of fraud.\n",
        "\n",
        "* **Transaction Velocity :-** Track the frequency and timing of transactions made by individual users. Rapid or abnormal increases in transaction velocity might suggest fraudulent behavior.\n",
        "\n",
        "* **Payment Method :-** Consider the payment method used for the transaction, such as credit/debit cards, digital wallets, UPI, or net banking. Certain payment methods may be associated with higher fraud risks.\n",
        "\n",
        "* **User Interaction Data :-** Gather additional data related to user interactions, such as login patterns, session duration, navigation behavior, and clickstream data. Unusual user interactions or high-risk actions can raise fraud alerts.\n",
        "\n",
        "* **Behavioral Biometrics :-** Analyze user-specific behavioral patterns, such as typing speed, mouse movements, touchscreen gestures, or device orientation. Any significant deviations from normal behavioral patterns may indicate fraud.\n",
        "\n",
        "* **Blacklists and Whitelists :-** Maintain lists of known fraudulent entities (blacklists) and trusted entities (whitelists). Compare transactions against these lists to identify suspicious or trusted entities.\n",
        "\n",
        "* **Risk Scores :-** Generate risk scores or fraud scores based on historical patterns and machine learning models. These scores can quantify the likelihood of a transaction being fraudulent and serve as an additional feature for detection."
      ],
      "metadata": {
        "id": "9dsIquM1hqgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5 :-** How do you measure the performance of the system?"
      ],
      "metadata": {
        "id": "bKoGkppki2AU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know Fraud Detection system is a supervised binary classification problem, here we have only two classes fraudulent and non fraudulent. To measure the performance of a fraud detection system for detecting fraudulent transactions in the BharatPe platform, several evaluation metrics can be utilized. Here are some commonly used metrics:\n",
        "we can use following metics : -\n",
        "\n",
        "* **Confusion Matrix: -** A confusion matrix provides a detailed breakdown of the model's performance, showing the number of true positives, true negatives, false positives, and false negatives. It can be used to calculate various performance metrics like precision, recall, accuracy, and F1 score.\n",
        "\n",
        "* **Accuracy: -** Accuracy is how many are correct from total prediction. Accuracy measures the overall correctness of the fraud detection system. As we know for this our dataset can be class imbalance because their are most of the transaction is not fraudulent and only some of the transactio will be fraudulent. So for such imbalance dataset accuracy alone may not be sufficient.\n",
        "\n",
        "* **Precision: -** Precision focuses on the proportion of correctly identified fraudulent transactions among all transactions flagged as fraudulent. It is calculated as the ratio of true positives (correctly identified fraudulent transactions) to the sum of true positives and false positives (non-fraudulent transactions incorrectly flagged as fraudulent). Higher precision indicates a lower rate of false positives.\n",
        "\n",
        "* **Recall : -** Recall measures the ability of the fraud detection system to identify all actual fraudulent transactions. It is calculated as the ratio of true positives to the sum of true positives and false negatives (fraudulent transactions incorrectly labeled as non-fraudulent). Higher recall indicates a lower rate of false negatives.\n",
        "\n",
        "* **F1 Score: -** The F1 score is the harmonic mean of precision and recall and provides a balanced measure of both metrics. It considers both the ability to identify fraudulent transactions (recall) and the accuracy of those identifications (precision). F1 score is especially useful when dealing with imbalanced datasets.\n",
        "\n",
        "* **ROC and AUC curve: -** The ROC curve is a graphical representation of the trade-off between true positive rate (recall) and false positive rate at various classification thresholds. AUC measures the overall performance of the model across all possible thresholds. Higher AUC values indicate better performance."
      ],
      "metadata": {
        "id": "UeKHqYoHjFcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer the following 15 questions:"
      ],
      "metadata": {
        "id": "-toTqfujU_12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. What are the  assumptions of  Linear Regression?**"
      ],
      "metadata": {
        "id": "XIFYb0sZK-1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are the assumption of Linear Regression\n",
        "\n",
        "*  **Linearity:** The relationship between the dependent and independent variables is linear.\n",
        "\n",
        "* **Independence:** The observations are independent of each other.\n",
        "\n",
        "* **Homoscedasticity:** The variance of the errors is constant across all levels of the independent variables.\n",
        "\n",
        "* **Normality:** The errors follow a normal distribution.\n",
        "No multicollinearity: The independent variables are not highly correlated with each other.\n",
        "\n",
        "* **No endogeneity:** There is no relationship between the errors and the independent variables."
      ],
      "metadata": {
        "id": "xVDdT_TvPsu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Explain how  Decision Tree Algorithm works? What is pruning in decision trees and what are its types?**"
      ],
      "metadata": {
        "id": "TUfMdQ4BLVRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision is a supervised learning algorithm that builds a tree-like model of decisions and their possible consequences. It is popular ML Algorithm used for both classification and Regression.\n",
        "\n",
        "Let's go through the Working of Decision Tree Algorithm\n",
        "\n",
        "* The algorithm starts with the entire dataset as the root node.\n",
        "\n",
        "* It selects the best attribute (feature) to split the data based on a certain criterion. Common criteria include Gini impurity and information gain.\n",
        "\n",
        "* The selected attribute is used to split the dataset into smaller subsets or branches.\n",
        "\n",
        "*  This process is recursively applied to each resulting subset, creating a tree structure until a stopping criterion is met. This criterion could be a certain depth limit, a minimum number of instances in a node, or the lack of further improvement in the impurity measure.\n",
        "\n",
        "### Pruning in Decision Tree :-\n",
        "Pruning is a technique used to reduce overfitting by removing unnecessary branches and nodes from the tree decision tree tend to overfit the training data that's mean it become overly complez and perform poorly on unseen data.\n",
        "\n",
        "Pruning is divided into two types (i) Pre Pruning (ii) Post Pruning\n",
        "\n",
        "**Pre Pruning :-** Pre-pruning involves setting stopping criteria before the tree construction begins.\n",
        "*  Setting a maximum depth for the tree.\n",
        "*  Specifying a minimum number of instances required in a leaf node.\n",
        "*  Limiting the number of instances needed to split a node.\n",
        "*  Defining a threshold for the improvement in impurity measure required for a split to occur.\n",
        "\n",
        "\n",
        "**Post Pruning :-**\n",
        "\n",
        "Post-pruning, also known as backward pruning or cost-complexity pruning, involves constructing the complete decision tree first and then selectively removing branches.It removing a node or branch improves the performance or does not significantly reduce it, the pruning occurs by replacing the removed section with a leaf node."
      ],
      "metadata": {
        "id": "cLnkIWRFPtZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. What is  Random Forest algorithm? How does random forest  reduce overfitting?**"
      ],
      "metadata": {
        "id": "4Psn07f8LavD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Random Forest algorithm is an ensemble learning method that combines multiple decision trees to make predictions. It is widely used for both classification and regression tasks and is known for its robustness and effectiveness."
      ],
      "metadata": {
        "id": "lFLrCDrranUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest reduces overfitting through several mechanisms:\n",
        "\n",
        "**Random Sampling of Data**\n",
        "\n",
        "* Random Forest uses bootstrap sampling, which involves randomly selecting subsets of the training data with replacement.\n",
        "\n",
        "* Each decision tree in the Random Forest is trained on a different bootstrap sample.\n",
        "\n",
        "* This random sampling introduces diversity in the training process, as each tree sees a slightly different perspective of the overall dataset.\n",
        "\n",
        "* By training on different subsets of the data, Random Forest reduces the likelihood of overfitting to specific instances or outliers in the training set.\n",
        "\n",
        "**Random Feature Selection :-**\n",
        "\n",
        "* Random Forest performs random feature selection at each node of the decision tree.\n",
        "\n",
        "* When splitting a node, only a subset of features is considered as potential candidates for the best split.\n",
        "\n",
        "* The number of features considered at each node is typically the square root of the total number of features.\n",
        "\n",
        "* Randomly selecting a subset of features ensures that different trees focus on different subsets of features.\n",
        "\n",
        "* It reduces the risk of overfitting to noise or irrelevant features in the dataset."
      ],
      "metadata": {
        "id": "ai91VLJ8bVot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. What is the difference between Precision and Recall? Explain with example**"
      ],
      "metadata": {
        "id": "jtKqAJkwLnxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and Recall are the two term which is used in binary classification task to measure the performance of the model.\n",
        "\n",
        "####**Precision**\n",
        "\n",
        "Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. It focuses on the accuracy of the positive predictions.\n",
        "\n",
        "Precision = (True Positives) / (True Positives + False Positives)\n",
        "\n",
        "* True Positives (TP): The number of instances correctly predicted as positive.\n",
        "\n",
        "* False Positives (FP): The number of instances incorrectly predicted as positive when they are actually negative.\n",
        "\n",
        "Let's consider a spam email classifier. Out of 100 emails predicted as spam, 95 are actually spam (true positives), and 5 are not spam (false positives). The precision would be:\n",
        "\n",
        "Precision = 95 / (95 + 5) = 0.95\n",
        "\n",
        "This means that 95% of the emails predicted as spam are actually spam."
      ],
      "metadata": {
        "id": "0jXuznbsdIv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Recall**\n",
        "\n",
        "Recall also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of all actual positive instances. It focuses on the ability of the model to identify positive instances.\n",
        "\n",
        "Recall = (True Positives) / (True Positives + False Negatives)\n",
        "\n",
        "* True Positives (TP): The number of instances correctly predicted as positive.\n",
        "* False Negatives (FN): The number of instances incorrectly predicted as negative when they are actually positive.\n",
        "\n",
        "let's assume that there are 100 actual spam emails, and the model correctly identifies 90 of them as spam (true positives). However, it incorrectly labels 10 spam emails as not spam (false negatives). The recall would be:\n",
        "\n",
        "Recall = 90 / (90 + 10) = 0.9\n",
        "\n",
        "This means that the model can capture 90% of the actual spam emails."
      ],
      "metadata": {
        "id": "i65yTkTNfJfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. What is KMeans?  How do you find optimal value of k  in KMeans algorithm?**"
      ],
      "metadata": {
        "id": "IxSqWBOQLtJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "K-Means is an unsupervised machine learning algorithm used for clustering, which is the task of grouping similar data points together. It aims to partition a given dataset into k clusters, where each data point belongs to the cluster with the nearest mean value. Here's an explanation of K-Means and how to find the optimal value of k:\n",
        "\n",
        "**K-Means Algorithm :-**\n",
        "\n",
        "* Initialize k cluster centroids randomly.\n",
        "\n",
        "* Assign each data point to the nearest centroid based on the Euclidean distance.\n",
        "* Recalculate the centroids by taking the mean of all the data points assigned to each centroid.\n",
        "\n",
        "* Repeat the assignment and centroid update steps until convergence (when the assignments no longer change significantly or a predefined number of iterations is reached).\n",
        "\n",
        "\n",
        "**Optimal Value of k :-**\n",
        "We can find optimal value of k by the the following method\n",
        "\n",
        "###**Silhouette Score :**\n",
        "\n",
        "* The Silhouette score is one of the method to evaluate clustering quality for different values of k.\n",
        "\n",
        "* For each data point, it measures the average distance between the data point and all other points within its own cluster (intra-cluster distance) and the average distance between the data point and all points in the nearest neighboring cluster (inter-cluster distance).\n",
        "\n",
        "* Calculate the Silhouette score for each value of k and choose the k value that maximizes the score.\n",
        "Higher Silhouette scores indicate better-defined and well-separated clusters.\n",
        "\n",
        "###**Elbow Method :-**\n",
        "\n",
        "* The Elbow Method is a common approach to estimate the optimal k value based on the distortion or inertia.\n",
        "\n",
        "* Distortion measures the sum of squared distances between each data point and\n",
        "its assigned centroid.\n",
        "\n",
        "* Iterate over different values of k and calculate the distortion for each value.\n",
        "Plot the distortion values against the corresponding k values.\n",
        "Look for the \"elbow\" point in the plot, where the distortion starts to decrease less significantly.\n",
        "\n",
        "* The elbow point indicates a good trade-off between compactness within clusters and separation between clusters, suggesting the optimal k value.\n"
      ],
      "metadata": {
        "id": "vtd8_j_bgVWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. What is Confusion Matrix?**"
      ],
      "metadata": {
        "id": "j0nC1EquMCvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a 2x2 metrix used to evaluate the performance of a classification model. It provides a summary of the predicted and actual class labels for a set of data instances. The confusion matrix is particularly useful in binary classification tasks.\n",
        "\n",
        "where there are two possible classes: positive and negative. Let's break down the components of a confusion matrix:\n",
        "\n",
        "**True Positives (TP):**\n",
        "\n",
        "The number of instances that are correctly predicted as positive (belonging to the positive class).\n",
        "\n",
        "**True Negatives (TN):**\n",
        "\n",
        "The number of instances that are correctly predicted as negative (belonging to the negative class).\n",
        "\n",
        "**False Positives (FP):**\n",
        "\n",
        "The number of instances that are incorrectly predicted as positive (predicted as positive but actually belonging to the negative class, also known as a Type I error).\n",
        "\n",
        "**False Negatives (FN):**\n",
        "\n",
        "The number of instances that are incorrectly predicted as negative (predicted as negative but actually belonging to the positive class, also known as a Type II error)."
      ],
      "metadata": {
        "id": "HVTlY0rgi4cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. What is random sampling? Give some examples of some random sampling techniques.**"
      ],
      "metadata": {
        "id": "jG4TND7QMEy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random sampling is a technique by which we are taking a sample of the data from the given population and then comes to conclusion"
      ],
      "metadata": {
        "id": "HdbIBHKok4v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random sampling : -** Random sampling is a technique to select a subset of data point from large population of data. Random sampling insures that these is all data points have a equal chance to being selected from the population.\n",
        "\n",
        "These are some random sampling technique are\n",
        "\n",
        "* **Simple Random Sampling :-** It is also called as SRS. Simple random sampling is a sampling technique where, every member of the population has an equal chance of being selected. Our sampling frame should include all the whole population. For markting reseach selecting 100 employees from 1000 employees.\n",
        "\n",
        "\n",
        "* **Systemetic Sampling :-** Systemetic sampling is similar to simple random sampling, but it is easier to conduct. Every member of the population is listed with a number, but instead of randomly generated numbers, individuals are choosen at the reguler inteval.\n",
        "\n",
        "\n",
        "* **Stratified Random Sampling :-** Stratified sampling involves dividing the population into sub - population based on some characteristics. It allows us to draw more precise conclusions by ensuring that every sub-group is properly represented in the sample. This subgroup called 'Strata'.\n",
        "\n",
        "\n",
        "* **Cluster Sampling :-** Cluster sampling also involves dividing the population into subgroup, but each sub-group should have similer characteristics to the whole sample, instead of sampling individual from each sub-group , you randomly select entire sub-groups."
      ],
      "metadata": {
        "id": "8Td8SVn1yOls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. What is the difference between tokenisation, lemmatization and stemming?**"
      ],
      "metadata": {
        "id": "hb8VgtsoMMR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization, lemmatization, and stemming are techniques used in natural language processing (NLP) to process and normalize text data. While they all aim to transform words into a standardized format.\n",
        "\n",
        "**Tokenization :-**\n",
        "\n",
        "Tokenization is the process of breaking a text document or sentence into individual words or tokens. These tokens serve as the basic units of text that can be analyzed further. Tokenization typically involves removing punctuation, splitting on whitespace, and handling special cases like contractions and hyphenated words.\n",
        "\n",
        "**Lemmatization :-**\n",
        "\n",
        "Lemmatization aims to reduce words to their base or canonical form, known as the lemma. The lemma represents the dictionary or base form of a word, ignoring variations due to tense, pluralization, or conjugation. Lemmatization takes into account the part of speech (POS) of the word and produces meaningful lemmas.\n",
        "\n",
        "\n",
        "**Stemming :-**\n",
        "\n",
        "Stemming is a simpler and more heuristic-based approach compared to lemmatization. It involves removing or truncating word suffixes to obtain the word's root or stem. The stemming process often uses pattern-based rules rather than language semantics."
      ],
      "metadata": {
        "id": "_K93hUS_zndR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. How do you handle imbalance data? Mention some techniques.**"
      ],
      "metadata": {
        "id": "79n0bqAIMOCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Imbalanced data is a common problem in machine learning, where there are significantly more data points in one class than in another. This can lead to models that are biased towards the majority class, and that do not perform well on the minority class.\n",
        "\n",
        "There are a number of techniques that can be used to handle imbalanced data. Some of the most common techniques include:\n",
        "\n",
        "* **Resampling :-** This involves either oversampling the minority class or undersampling the majority class. Oversampling involves creating more data points for the minority class, while undersampling involves removing data points from the majority class.\n",
        "\n",
        "* **Cost-sensitive learning :-** This involves assigning different costs to different misclassifications. For example, you might assign a higher cost to misclassifying a minority class data point than to misclassifying a majority class data point.\n",
        "\n",
        "* **Ensemble learning :-** This involves training multiple models on the imbalanced data, and then combining the predictions of the models. This can help to reduce the bias towards the majority class.\n",
        "\n",
        "* **Data preprocessing :-** This involves transforming the data in a way that reduces the imbalance. For example, you might normalize the data or use a technique called SMOTE to oversample the minority class.\n",
        "\n",
        "The best technique to use for handling imbalanced data depends on the specific problem. However, resampling and cost-sensitive learning are two of the most common techniques.\n"
      ],
      "metadata": {
        "id": "O5rLeiEV0cXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. What is univariate and bivariate analysis? Mention some its techniques.**"
      ],
      "metadata": {
        "id": "98HHV5PzMTvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate and bivariate analyses are two fundamental techniques in data analysis that help in understanding the relationships and characteristics of variables.\n",
        "\n",
        "Univariate Analysis:\n",
        "\n",
        "Univariate analysis involves the examination and analysis of a single variable at a time. It focuses on summarizing and describing the distribution, central tendency, dispersion, and other characteristics of a variable. Some techniques used in univariate analysis include:\n",
        "\n",
        "* **Descriptive statistics :-** These include measures such as mean, median, mode, range, variance, and standard deviation, which provide insights into the central tendency and dispersion of a variable.\n",
        "\n",
        "* **Histograms :-** Histograms visualize the frequency distribution of a variable by dividing it into bins and displaying the number of observations within each bin.\n",
        "\n",
        "* **Box plots :-** Box plots provide a graphical representation of the distribution of a variable, displaying its quartiles, outliers, and skewness.\n",
        "\n",
        "* **Probability density functions (PDF):** PDFs show the probability distribution of a variable by plotting the probability of each value or range of values.\n",
        "\n",
        "* **Bar charts :-** Bar charts represent categorical variables by showing the frequency or proportion of each category."
      ],
      "metadata": {
        "id": "-gVHI_DXcjC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. Describe Hypothesis Testing. How is the statistical significance of an insight assessed? What is Alpha in hypothesis testing?**"
      ],
      "metadata": {
        "id": "rRFTCJf6Mqh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis testing is a statistical method used to determine whether there is enough evidence to support a claim about a population. It involves making two mutually exclusive hypotheses, the null hypothesis and the alternative hypothesis. The null hypothesis is the statement that there is no difference between the population and the sample. The alternative hypothesis is the statement that there is a difference between the population and the sample.\n",
        "\n",
        "The statistical significance of an insight is assessed using a significance level, which is denoted by the Greek letter alpha (α). The significance level is the probability of rejecting the null hypothesis when it is actually true. A significance level of 0.05 means that there is a 5% chance of rejecting the null hypothesis when it is actually true.\n",
        "\n",
        "In hypothesis testing, the p-value is used to determine whether to reject the null hypothesis. The p-value is the probability of obtaining a sample statistic as extreme or more extreme than the one that was actually observed, assuming that the null hypothesis is true.\n",
        "\n",
        "The p-value is compared to the significance level. If the p-value is less than or equal to the significance level, then the null hypothesis is rejected. If the p-value is greater than the significance level, then the null hypothesis is not rejected."
      ],
      "metadata": {
        "id": "U0hl49Oaeu9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12. What is  the difference between Label encoding and One Hot Encoding?**"
      ],
      "metadata": {
        "id": "NGekudS4Mtug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label encoding and one-hot encoding are two common techniques used to transform categorical variables into a format that machine learning algorithms can handle.\n",
        "\n",
        "**Label Encoding :-**\n",
        "\n",
        "Label encoding involves assigning a unique numeric label to each unique category in a categorical variable. Each category is mapped to a numerical value, effectively converting it into a numerical representation. For example:\n",
        "* Category A: 0\n",
        "* Category B: 1\n",
        "* Category C: 2\n",
        "\n",
        "\n",
        "**One-Hot Encoding :-**\n",
        "\n",
        "One-hot encoding, also known as dummy encoding, creates binary features for each unique category in a categorical variable. Each category becomes a new binary feature, and for each observation.\n",
        "\n",
        "* Category A: [1, 0, 0]\n",
        "* Category B: [0, 1, 0]\n",
        "* Category C: [0, 0, 1]\n",
        "\n",
        "These are some differences between Label Encoding and One-Hot Encoding:\n",
        "\n",
        "* Label encoding represents categorical variables with integers, whereas one-hot encoding represents each category with binary features.\n",
        "\n",
        "* Label encoding assumes an ordinal relationship among the categories, while one-hot encoding treats the categories as independent.\n",
        "\n",
        "* Label encoding may be suitable for categorical variables with inherent order or ranking, while one-hot encoding is generally applicable to categorical variables without a meaningful order.\n",
        "\n",
        "* Label encoding results in a compact representation with a single feature, while one-hot encoding expands the feature space with multiple binary features."
      ],
      "metadata": {
        "id": "LrkO4_2RftZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. What is conditional probability? If I call 2 Ubers and 3 Lyfts, what is the probability that all the Lyfts arrive first?**"
      ],
      "metadata": {
        "id": "uVqTzVRAM7mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditional probability : -** Conditional probability is a measure of the probability of an event occurring given that another event has already occurred or is known to have occurred. It represents the probability of an outcome under a specific condition or context.\n",
        "\n",
        "The conditional probability of an event A given event B is denoted as P(A|B), read as \"the probability of A given B.\" It is calculated as:\n",
        "\n",
        "P(A|B) = P(A ∩ B) / P(B)\n",
        "\n",
        "**Probability that all the Lyfts arrive first : -** Let's say the total number of possible orderings of the 5 vehicles (2 Ubers and 3 Lyfts) and determine the favorable outcomes where all the Lyfts arrive first.\n",
        "\n",
        "The total number of possible is 5!\n",
        "\n",
        "all the Lyfts arrive first = 3!\n",
        "\n",
        "Similarly, total 2 Ubers combination is = 2!\n",
        "\n",
        "Therefore, the number of favorable outcomes is 3! * 2!\n",
        "\n",
        "Finally, the probability of all lyfts arrive first is,\n",
        "Probability = (3!*2!) / (5!)\n",
        "Probability = 1/10 = 0.1\n",
        "\n",
        "Therefore, the probability that all the Lyfts arrive first is 1/10 or 10%."
      ],
      "metadata": {
        "id": "eE8Bbud_hTVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. What are Outliers and how do you treat them?**"
      ],
      "metadata": {
        "id": "iSD3IFhUNIuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An outlier is a data point that is significantly different from the rest of the data points in a set. Outliers can be caused by a number of factors, such as data entry errors, measurement errors, or natural variation.\n",
        "\n",
        "There are a number of ways to treat outliers. Some common methods include:\n",
        "\n",
        "* **Deleting outliers :-** This is the simplest method, but it can also be the most dangerous. If the outlier is a valid data point, deleting it can skew the results of your analysis.\n",
        "\n",
        "* **Imputing outliers :-** This involves replacing the outlier with a more reasonable value. This can be done by using the mean, median, or mode of the data set, or by using a more sophisticated imputation method.\n",
        "\n",
        "* **Robust statistical methods :-** These methods are designed to be less sensitive to outliers. They can be used to calculate summary statistics, such as the mean and standard deviation, that are not affected by outliers.\n",
        "\n",
        "\n",
        "There are some of the reasons why outlier treatment is important:\n",
        "\n",
        "* Outliers can skew the results of your analysis.\n",
        "\n",
        "* Outliers can make it difficult to identify patterns in your data.\n",
        "\n",
        "* Outliers can make it difficult to make accurate predictions.\n",
        "\n",
        "\n",
        "There are some of the methods that can be used to detect outliers:\n",
        "\n",
        "* **Boxplot :-** A boxplot is a graphical representation of a data set that shows the distribution of the data, including the outliers.\n",
        "\n",
        "* **Z-score :-** The z-score is a measure of how far a data point is from the mean of the data set. Outliers are typically defined as data points that have a z-score greater than 3 or less than -3.\n",
        "\n",
        "* **Interquartile range (IQR) :-** The IQR is a measure of the variability of the data set. Outliers are typically defined as data points that are less than Q1 - 1.5 * IQR or greater than Q3 + 1.5 * IQR."
      ],
      "metadata": {
        "id": "9m6iSk4aWUGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **15. What is Naive Bayes Classifier Algorithm. What do you mean by ‘Naive’ in a Naive Bayes?**"
      ],
      "metadata": {
        "id": "hjmlljMFNSWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes classifier is a simple and powerful algorithm used for classification tasks in machine learning and statistics. It is based on the application of Bayes' theorem with the assumption of independence among the features.\n",
        "\n",
        "The algorithm is called \"Naive\" because it assumes that the features (variables) are conditionally independent given the class label. This is a strong and naive assumption because, in reality, many features may be dependent on each other. Nevertheless, despite this simplifying assumption, Naive Bayes classifiers often perform well in practice and are particularly effective for text classification and other high-dimensional datasets."
      ],
      "metadata": {
        "id": "mjQn18ciiibS"
      }
    }
  ]
}